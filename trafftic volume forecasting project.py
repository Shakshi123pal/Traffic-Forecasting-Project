# -*- coding: utf-8 -*-
"""trafficForecastingproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H6abdQCk7-LPYmr-KWmjT9SBLgmu5Aie
"""

!nvidia-smi

import numpy as np
import pandas as pd

data=pd.read_csv("traffic_Dataset.csv")

data.head()
#describing the data
#traffic volume is the vehicle count
#average speed is in km/h
#travel time index=actual travel time/free flow travel time

data.shape

data.info()

data.dtypes

data.isnull().sum()

data.describe()

data['Date']=data['Date'].apply(lambda x:"-".join(x.split("-")[::-1]))

data['Date']=data['Date'].str.strip()

data['Date']=pd.to_datetime(data['Date'], format='%d-%m-%Y')
data['Year']=data['Date'].dt.year
data['Month']=data['Date'].dt.month
data['Day']=data['Date'].dt.day
data['Is Weekend']=data['Date'].dt.weekday.isin([5,6])
data

import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('dark_background')
numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns

# Calculate the correlation matrix using only the numeric columns
plt.figure(figsize=(10, 5))
corr_matrix = data[numeric_columns].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

(data['Environmental Impact'] == data['Traffic Volume']).all()

data[['Traffic Volume', 'Environmental Impact']].corr()

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(data[['Traffic Volume']], data['Environmental Impact'])

print(f"Coefficient: {model.coef_[0]}")
print(f"Intercept: {model.intercept_}")

import seaborn as sns
import matplotlib.pyplot as plt

sns.scatterplot(x='Traffic Volume', y='Environmental Impact', data=data)
plt.title("Traffic Volume vs Environmental Impact")
plt.show()

# there is almost negligible correlation between traffic volume and parking usage, public transport usage and traffic signal compliance
# also remove Environmental Impact because it is derived from target column — if I keep it then it will cause data leakage in model training
data = data.drop(['Parking Usage', 'Public Transport Usage', 'Traffic Signal Compliance', 'Environmental Impact'], axis=1)

data.head()

#normalising the traffic volume using min -max scaler
from sklearn.preprocessing import MinMaxScaler

train_data=data[(data['Year']==2022) | (data['Year']==2023)]
test_data=data[data['Year']==2024]

scaler = MinMaxScaler()
train_data['Traffic Volume'] = scaler.fit_transform(train_data[['Traffic Volume']])
test_data['Traffic Volume'] = scaler.transform(test_data[['Traffic Volume']])

# we are doing the heatmap below then we are apply scaler to it after splitting like it:
data.loc[train_data.index, 'Traffic Volume'] = train_data['Traffic Volume']
data.loc[test_data.index, 'Traffic Volume'] = test_data['Traffic Volume']

import matplotlib.pyplot as plt
import seaborn as sns

heatmap_data = data.pivot_table(index='Road/Intersection Name',
                              columns='Area Name',
                              values='Traffic Volume',
                              aggfunc='sum',
                              fill_value=0)


plt.style.use('dark_background')

plt.figure(figsize=(12, 5))
sns.heatmap(heatmap_data,
            cmap='inferno',
            linewidths=0.5,
            linecolor='gray',
            annot=True,
            fmt='.0f',
            annot_kws={"size": 8, "color": "cyan","weight":"bold"})

plt.title("Traffic Volume Heatmap: Road/Intersection vs Area", fontsize=14)
plt.xlabel("Area Name")
plt.ylabel("Road/Intersection Name")
plt.tight_layout()
plt.show()

# so there you see Top 5 Busiest Road-Area Combinations
# Rank	Road/Intersection	   Area	Volume
#  1	 Sony World Junction	  Koramangala	376
# 2	   CMH Road	              Indiranagar	360
# 3	   100 Feet Road	          Indiranagar	352
# 4	   Anil Kumble Circle	    M.G. Road	347
# 5	   Trinity Circle	        M.G. Road	341

import plotly.express as px


# Sunburst chart: Area > Intersection > Volume
fig = px.sunburst(data,
                  path=['Area Name', 'Road/Intersection Name'],   #  defines the hierarchy
                  values='Traffic Volume',                      # tells Plotly to use traffic volume as the size
                  color='Traffic Volume',                       # also maps traffic volume to color intensity
                  color_continuous_scale='viridis',             # sets the color scheme
                  title='Traffic Volume Distribution: Area → Intersection')

# Optional: dark theme look
fig.update_layout(
    template='plotly_dark',        # gives it a dark background for a modern look
    margin=dict(t=50, l=0, r=0, b=0)
)

# Show chart
fig.show()

data['Weather Conditions'].unique()

weather = data[data['Weather Conditions'].isin(['Clear','Overcast','Fog','Rain','Windy'])]
df = weather.groupby(['Year', 'Weather Conditions'])['Traffic Volume'].sum().reset_index()

pivot_df = df.pivot(index='Weather Conditions', columns='Year', values='Traffic Volume')
pivot_df = pivot_df[[2022, 2023]]

# Step 3: Plot grouped bar chart
label =['Clear','Overcast','Fog','Rain','Windy']
x = np.arange(len(label))  # x locations
width = 0.35  # width of the bars

fig, ax = plt.subplots(figsize=(10,6))
bars1 = ax.bar(x - width/2, pivot_df[2022], width, label='2022', color='skyblue')
bars2 = ax.bar(x + width/2, pivot_df[2023], width, label='2023', color='orange')

# Add labels
ax.set_xlabel('Weather')
ax.set_ylabel('Traffic Volume Sum')
ax.set_title('')
ax.set_xticks(x)
ax.set_xticklabels(label)
ax.legend()

# Add values on top
for bar in bars1 + bars2:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, height, f'{height:.0f}',
            ha='center', va='bottom', fontsize=8)

plt.tight_layout()
plt.show()

fig = px.histogram(data, x='Weather Conditions', y='Traffic Volume', color='Area Name',
                   barmode='stack', title="Stacked Bar Chart of Traffic Volume by Weather Conditions")
fig.update_layout(
    template='plotly_dark',
    margin=dict(t=50, l=0, r=0, b=0)
)

# Show chart
fig.show()

data.groupby(['Is Weekend'])[['Traffic Volume']].sum()

import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Group the data
grouped_data = data.groupby(['Year', 'Month'])[['Average Speed', 'Travel Time Index']].mean().reset_index()
traffic_sum = data.groupby(['Year', 'Month'])[['Traffic Volume']].sum().reset_index()
grouped_data = grouped_data.merge(traffic_sum, on=['Year', 'Month'])


# Step 2: Plot using FacetGrid
g = sns.FacetGrid(grouped_data, col="Year", col_wrap=2, height=6)
g.map_dataframe(
    sns.scatterplot,
    x="Average Speed",
    y="Travel Time Index",
    hue="Traffic Volume",
    palette="viridis",
    size="Traffic Volume",
    sizes=(30, 200)
)

# Step 3: Add titles and legends
g.add_legend()
g.set_titles(col_template="Year: {col_name}")
g.set_axis_labels("Average Speed", "Travel Time Index")
plt.subplots_adjust(top=0.9)
g.fig.suptitle("Year-wise Traffic Scatter Plots", fontsize=16)

plt.show()

data = data[(data['Year'].isin([2022, 2023])) & (data['Month'] <= 12)]

daily_avg = data.groupby(['Year', 'Month', 'Day'])['Traffic Volume'].sum().reset_index()

months = [1, 2, 3, 4, 5, 6,7,8,9,10,11,12]
month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun','July','Aug','Sep','Oct','Nov','Dec']

fig, axes = plt.subplots(12, 1, figsize=(20, 80), sharey=True)
axes = axes.flatten()

for i, month in enumerate(months):
    ax = axes[i]
    month_data = daily_avg[daily_avg['Month'] == month]
    sns.lineplot(data=month_data, x='Day', y='Traffic Volume', hue='Year', marker='o', ax=ax)
    ax.set_title(month_names[i])
    ax.set_xlabel('Day of Month')
    ax.set_ylabel('Traffic Volume')
    ax.legend(title='Year')

plt.suptitle('Monthly Daily Traffic Volume Comparison (2022 vs 2023)', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

data = data[(data['Year'].isin([2022, 2023])) & (data['Month'] <= 12)]

daily_avg = data.groupby(['Year', 'Month', 'Day'])['Congestion Level'].sum().reset_index()

months = [1, 2, 3, 4, 5, 6,7,8,9,10,11,12]
month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun','July','Aug','Sep','Oct','Nov','Dec']

fig, axes = plt.subplots(12, 1, figsize=(20, 80), sharey=True)
axes = axes.flatten()

for i, month in enumerate(months):
    ax = axes[i]
    month_data = daily_avg[daily_avg['Month'] == month]
    sns.lineplot(data=month_data, x='Day', y='Congestion Level', hue='Year', marker='o', ax=ax)
    ax.set_title(month_names[i])
    ax.set_xlabel('Day of Month')
    ax.set_ylabel('Traffic Volume')
    ax.legend(title='Year')

plt.suptitle('Monthly Daily Congestion level  Comparison (2022 vs 2023)', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

## autoformer --handles seasonality ,trends, Series decomposition ,clear seasonal pattern
## informer -- fast for long sequences ,probSparse attention,need efficient predication
## spacetimeformer --spatiotemporal dependencies,location-time attention

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf

# Step 1: Prepare your original daily total traffic
data = data[(data['Year'].isin([2022, 2023])) & (data['Month'] <= 12)]
daily_total = data.groupby('Date')['Traffic Volume'].sum()

# Step 2: Scale the data
scaler1 = MinMaxScaler()
scaled = scaler1.fit_transform(daily_total.values.reshape(-1, 1))

# Step 3: Create sequences
def create_sequences(series, seq_len=30):
    X, y = [], []
    for i in range(len(series) - seq_len):
        X.append(series[i:i+seq_len])
        y.append(series[i+seq_len])
    return np.array(X), np.array(y)

seq_length = 30
X, y = create_sequences(scaled, seq_length)
X = X.reshape((X.shape[0], seq_length, 1))  # [samples, timesteps, features]

# Step 4: Define Transformer model
class TimeSeriesTransformer(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.embedding = tf.keras.layers.Dense(64)
        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=16)
        self.flatten = tf.keras.layers.Flatten()
        self.dense1 = tf.keras.layers.Dense(64, activation='relu')
        self.output_layer = tf.keras.layers.Dense(1)

    def call(self, inputs):
        x = self.embedding(inputs)
        x = self.attention(x, x)
        x = self.flatten(x)
        x = self.dense1(x)
        return self.output_layer(x)

# Step 5: Train the model
model = TimeSeriesTransformer()
model.compile(optimizer='adam', loss='mse')
model.fit(X, y, epochs=20, verbose=1)

# Step 6: Forecast for 365 days (2024)
forecast_scaled = []
input_seq = list(scaled[-seq_length:])  # last 30 days

for _ in range(365):
    x_input = np.array(input_seq[-seq_length:]).reshape((1, seq_length, 1)).astype(np.float32)
    pred = model.predict(x_input, verbose=0)[0][0]
    forecast_scaled.append(pred)
    input_seq.append([pred])

# Step 7: Inverse transform forecast
forecast = scaler1.inverse_transform(np.array(forecast_scaled).reshape(-1, 1)).flatten()

# Step 8: Create forecast dataframe
forecast_dates = pd.date_range(start='2024-01-01', periods=365, freq='D')
forecast_df = pd.DataFrame({
    'Date': forecast_dates,
    'Traffic Volume': forecast,
})
forecast_df['Year'] = forecast_df['Date'].dt.year
forecast_df['Month'] = forecast_df['Date'].dt.month
forecast_df['Day'] = forecast_df['Date'].dt.day

#making date column in actual_df
actual_df = test_data[test_data['Year'] == 2024]
actual_df= actual_df.groupby(['Year', 'Month', 'Day'])['Traffic Volume'].sum().reset_index()
actual_df['Date'] = pd.to_datetime(actual_df[['Year', 'Month', 'Day']])

actual_df=actual_df[['Date','Traffic Volume']]
forecast_df=forecast_df[['Date','Traffic Volume']]



#joining actual and forecast on date
# Step 1: Rename columns so they don't clash
actual_df = actual_df.rename(columns={'Traffic Volume': 'Actual Traffic'})
forecast_df = forecast_df.rename(columns={'Traffic Volume': 'Predicted Traffic'})

result_df = pd.merge(actual_df, forecast_df, on='Date', how='inner')

# Inverse transform
actual_unscaled = scaler.inverse_transform(result_df[['Actual Traffic']])
predicted_unscaled = scaler.inverse_transform(result_df[['Predicted Traffic']])
result_df['Actual Traffic'] = actual_unscaled
result_df['Predicted Traffic'] = predicted_unscaled

df['Month']=df['Date'].dt.month
df['Day']=df['Date'].dt.day

df=df.groupby(['Month','Day'])[['Actual Traffic','Predicted Traffic']].mean()

df

df=df.reset_index()

import matplotlib.pyplot as plt

# Month names for nice titles
month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
               'Jul', 'Aug']

# Create subplots — 12 for 12 months
fig, axes = plt.subplots(8, 1, figsize=(20, 30))
axes = axes.flatten()  # To access subplots with a single index

# Loop through each month
for i, month in enumerate(range(1, 9)):
    ax = axes[i]

    month_df = df[df['Month'] == month].sort_values(by='Day')

    ax.plot(month_df['Day'], month_df['Actual Traffic'], label='Actual', marker='o')
    ax.plot(month_df['Day'], month_df['Predicted Traffic'], label='Predicted', marker='x')



    for x, y in zip(month_df['Day'], month_df['Predicted Traffic']):
        ax.annotate(f'{y:.0f}', (x, y), textcoords="offset points", xytext=(0, 5), ha='center', fontsize=8)



    ax.set_title(f'{month_names[i]}: Daily Traffic')
    ax.set_xlabel('Day')
    ax.set_ylabel('Traffic Volume')
    ax.legend()


plt.tight_layout()
plt.suptitle('Actual vs Predicted Daily Traffic (Month-wise)', fontsize=20, y=1.02)
plt.show()

df=data.groupby(['Date','Area Name'])[['Traffic Volume','Congestion Level']].sum()

df=pd.DataFrame(df)
df=df.reset_index()
df.head()

data_pivot1 = df.pivot(index='Date', columns='Area Name', values='Traffic Volume')
data_pivot2= df.pivot(index='Date', columns='Area Name', values='Congestion Level')

data_pivot1 = data_pivot1.fillna(0)
data_pivot2 = data_pivot2.fillna(0)
data_pivot1

data_pivot2

!pip install einops

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

# AutoCorrelation Layer (Autoformer-specific)
class AutoCorrelation(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, queries, keys, values):
        # simplified correlation over time dimension
        correlation = torch.einsum('bth,bsh->bts', queries, keys) / queries.size(-1) ** 0.5
        weights = F.softmax(correlation, dim=-1)
        return torch.einsum('bts,bsh->bth', weights, values)

# Encoder block
class AutoformerEncoderBlock(nn.Module):
    def __init__(self, d_model, dropout=0.1):
        super().__init__()
        self.auto_corr = AutoCorrelation()
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Linear(d_model, d_model)
        )
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        attn = self.auto_corr(x, x, x)
        x = self.norm1(x + self.dropout(attn))
        ff_out = self.ff(x)
        x = self.norm2(x + self.dropout(ff_out))
        return x

# Main Autoformer model
class Autoformer(nn.Module):
    def __init__(self, seq_len=30, d_model=64, num_layers=2):
        super().__init__()
        self.embedding = nn.Linear(1, d_model)
        self.encoder = nn.Sequential(*[
            AutoformerEncoderBlock(d_model) for _ in range(num_layers)
        ])
        self.decoder = nn.Sequential(
            nn.Flatten(),
            nn.Linear(seq_len * d_model, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):  # x: [B, 30, 1]
        x = self.embedding(x)  # -> [B, 30, d_model]
        x = self.encoder(x)
        return self.decoder(x)  # -> [B, 1]

def create_sequences(series, seq_len=30):
    X, y = [], []
    for i in range(len(series) - seq_len):
        X.append(series[i:i+seq_len])
        y.append(series[i+seq_len])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled, seq_len=30)
X = X.reshape((X.shape[0], 30, 1))  # [samples, 30, 1]

print(type(y), y.shape)

X_tensor = torch.tensor(X, dtype=torch.float32)

if len(y.shape) == 1:
    y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)
else:
    y_tensor = torch.tensor(y, dtype=torch.float32)

print(type(y), y.shape)

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

# 📦 Train-val split
train_size = int(0.8 * len(X_tensor))
X_train, X_val = X_tensor[:train_size], X_tensor[train_size:]
y_train, y_val = y_tensor[:train_size], y_tensor[train_size:]

# 📦 Dataset + Dataloader
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# ✅ Define Autoformer (if not defined yet)
# We'll define a simple placeholder Autoformer here — you can replace it with your full version later
class Autoformer(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, batch_first=True)
        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)
        self.input_proj = nn.Linear(1, 64)
        self.output_proj = nn.Linear(64 * 30, 1)

    def forward(self, x):
        x = self.input_proj(x)                # [B, 30, 64]
        x = self.encoder(x)                   # [B, 30, 64]
        x = x.reshape(x.shape[0], -1)         # [B, 30*64]
        return self.output_proj(x)            # [B, 1]

# 🎯 Model, loss, optimizer
model = Autoformer()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 🔁 Training loop
for epoch in range(20):
    model.train()
    total_loss = 0.0
    for xb, yb in train_loader:
        pred = model(xb)
        loss = criterion(pred, yb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}")

# import torch.nn.functional as F

class Informer(nn.Module):
    def __init__(self, seq_len=30, d_model=64, n_heads=4):
        super().__init__()
        self.seq_len = seq_len
        self.input_proj = nn.Linear(1, d_model)

        # Encoder
        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, batch_first=True
        )
        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)

        # Decoder: use last encoder token or mean-pooled encoder output
        self.decoder = nn.Sequential(
            nn.Linear(d_model * seq_len, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        x = self.input_proj(x)      # [B, 30, d_model]
        x = self.encoder(x)         # [B, 30, d_model]
        x = x.reshape(x.size(0), -1)  # Flatten: [B, 30*d_model]
        out = self.decoder(x)       # [B, 1]
        return out

# Replace model = Autoformer() with:
model = Informer(seq_len=30, d_model=64, n_heads=4)

# Keep the rest identical:
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(20):
    model.train()
    total_loss = 0.0
    for xb, yb in train_loader:
        pred = model(xb)
        loss = criterion(pred, yb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}")

# Make sure Autoformer model is trained before this
model.eval()

# 🔍 Predict on validation data
with torch.no_grad():
    y_pred = model(X_val).squeeze().cpu().numpy()
    y_true = y_val.squeeze().cpu().numpy()

# 🔁 Inverse transform using the same scaler1 used in preprocessing
y_pred_unscaled = scaler1.inverse_transform(y_pred.reshape(-1, 1)).flatten()
y_true_unscaled = scaler1.inverse_transform(y_true.reshape(-1, 1)).flatten()

# 📊 Calculate Evaluation Metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error

mae = mean_absolute_error(y_true_unscaled, y_pred_unscaled)
rmse = mean_squared_error(y_true_unscaled, y_pred_unscaled)

print(f'📈 Autoformer Evaluation:')
print(f'➡️ MAE  (Mean Absolute Error): {mae:.2f}')
print(f'➡️ RMSE (Root Mean Squared Error): {rmse:.2f}')

model = Informer(seq_len=30, d_model=64, n_heads=4)
model.eval()
with torch.no_grad():
    y_val_pred_scaled = model(X_val).squeeze().numpy()

# Convert predictions and ground truth back to original scale
y_val_pred = scaler1.inverse_transform(y_val_pred_scaled.reshape(-1, 1)).flatten()
y_val_actual = scaler1.inverse_transform(y_val.numpy().reshape(-1, 1)).flatten()
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

mae = mean_absolute_error(y_val_actual, y_val_pred)
rmse = mean_squared_error(y_val_actual, y_val_pred)

print(f"📈 Informer Evaluation:")
print(f"➡️ MAE  (Mean Absolute Error): {mae:.2f}")
print(f"➡️ RMSE (Root Mean Squared Error): {rmse:.2f}")

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

mse = mean_squared_error(result_df['Actual Traffic'], result_df['Predicted Traffic'])
rmse = np.sqrt(mse)  # Manual RMSE
mae = mean_absolute_error(result_df['Actual Traffic'], result_df['Predicted Traffic'])
mape = np.mean(np.abs((result_df['Actual Traffic'] - result_df['Predicted Traffic']) / result_df['Actual Traffic'])) * 100
print(f"Vanilla evaluation:")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")

import matplotlib.pyplot as plt
import numpy as np

# Model names
models = ['Vanilla Transformer', 'Informer', 'Autoformer']

# Evaluation values (use unscaled values you got)
rmse_values = [53170.21, 8.48, 0.57]
mae_values = [42246.37, 2.81, 0.59]

# Set bar width
bar_width = 0.35
x = np.arange(len(models))  # label locations

# Create figure
fig, ax = plt.subplots(figsize=(10, 6))

# Plot bars
bars1 = ax.bar(x - bar_width/2, rmse_values, bar_width, label='RMSE', color='skyblue')
bars2 = ax.bar(x + bar_width/2, mae_values, bar_width, label='MAE', color='lightcoral')

# Add labels and title
ax.set_xlabel('Models')
ax.set_ylabel('Error Value')
ax.set_title('Model Performance Comparison (Unscaled)')
ax.set_xticks(x)
ax.set_xticklabels(models, rotation=15)
ax.legend()

# Annotate bars with values
for bar in bars1 + bars2:
    height = bar.get_height()
    ax.annotate(f'{height:.2f}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),  # offset
                textcoords="offset points",
                ha='center', va='bottom', fontsize=8)

plt.tight_layout()
plt.show()

